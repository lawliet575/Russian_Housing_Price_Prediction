{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import xgboost as xgb \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\Ikhlas University\\\\Semester 5\\\\IML\\\\Kaggle Challenge_2\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1) #drops columns\n",
    "df.dropna(how='all')\n",
    "df.isnull().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['product_type'] = encoder.fit_transform(df['product_type'])\n",
    "df['ecology'] = encoder.fit_transform(df['ecology'])\n",
    "df['ecology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='sub_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion' , 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', \n",
    "'big_road1_1line',  'railroad_1line', 'water_1line']\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    df[col] = df[col].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df.iloc[:, 0:270]  # All columns except the last one cahnge to 27- if subarea dropped\n",
    "y = df.iloc[:, 270]   # Only the last column\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat this activity multiple times and record the R2 and MSE for each run\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_model=ExtraTreesRegressor(n_estimators=2000, min_samples_split=4, bootstrap=False, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_model.fit(X_train,y_train)\n",
    "y_predict = rf_model.predict(X_test)\n",
    "\n",
    "#print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_predict))\n",
    "print(\"Root Mean squared error: %.2f\" % root_mean_squared_error(y_test, y_predict))\n",
    "print(\"R-squared = %.3f\" % r2_score(y_test, y_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"D:\\\\Ikhlas University\\\\Semester 5\\\\IML\\\\Kaggle Challenge_2\\\\test.csv\")\n",
    "rowID = test_data['row ID']\n",
    "test_data = test_data.drop(columns = 'row ID')\n",
    "test_data = test_data.drop(columns = 'sub_area')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.dropna(axis=1) #drops columns\n",
    "test_data.dropna(how='all')\n",
    "test_data.isnull().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder = LabelEncoder()\n",
    "test_data['product_type'] = test_encoder.fit_transform(test_data['product_type'])\n",
    "test_data['ecology'] = test_encoder.fit_transform(test_data['ecology'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion' , 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', \n",
    "'big_road1_1line',  'railroad_1line', 'water_1line']\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    test_data[col] = test_data[col].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO CHANGE THE MODEL NAME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf_model.predict(test_data)\n",
    "finaldf = pd.concat([rowID, pd.DataFrame(prediction, columns=['price_doc'])], axis=1)\n",
    "finaldf.to_csv(\"D:\\\\Ikhlas University\\\\Semester 5\\\\IML\\\\Kaggle Challenge_2\\\\Result.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian House Price Prediction\n",
    "\n",
    "A detailed analysis of the models and feature preprocessing steps used for predicting house prices in Russia.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Model Evaluations](#model-evaluations)\n",
    "    - [Linear Regression](#linear-regression)\n",
    "    - [Polynomial Regression](#polynomial-regression)\n",
    "    - [K-Nearest Neighbors (KNN)](#k-nearest-neighbors-knn)\n",
    "    - [Regression Trees](#regression-trees)\n",
    "    - [Random Forest](#random-forest)\n",
    "    - [AdaBoost](#adaboost)\n",
    "    - [Gradient Boosting](#gradient-boosting)\n",
    "    - [XGBoost](#xgboost)\n",
    "    - [Neural Networks](#neural-networks)\n",
    "    - [Stacking](#stacking)\n",
    "2. [Feature Preprocessing](#feature-preprocessing)\n",
    "    - [Principal Component Analysis (PCA)](#principal-component-analysis-pca)\n",
    "    - [Feature Importance & Selection](#feature-importance--selection)\n",
    "    - [Scaling & Normalization](#scaling--normalization)\n",
    "    - [Handling Missing Values](#handling-missing-values)\n",
    "    - [Label Encoding](#label-encoding)\n",
    "3. [Challenges](#challenges)      \n",
    "4. [Winner Of the Competition](#the-winner-of-the-competition-extra-trees-regressor)   \n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluations\n",
    "\n",
    "### Linear Regression\n",
    "- **Performance**: RMSE = 13,283,371.97  \n",
    "- Experimented with solvers (`saga` and `lsqr`), but improvements were minimal.\n",
    "- **Conclusion**: Performed poorly compared to other models.\n",
    "\n",
    "---\n",
    "\n",
    "### Polynomial Regression\n",
    "- **Best Results**: Degree 2 with 75 principal components: RMSE = 13,695,385.56  \n",
    "- Increasing degree (e.g., 3) worsened performance significantly: RMSE = 17,716,343.26.  \n",
    "- **Conclusion**: Only marginally effective; overfitted with higher degrees.\n",
    "\n",
    "---\n",
    "\n",
    "### K-Nearest Neighbors (KNN)\n",
    "- **Best Results**: k = 50: RMSE = 12,786,347.29.  \n",
    "- Increasing neighbors to k = 150 improved accuracy but led to overfitting beyond this point.  \n",
    "- **Conclusion**: Better than Polynomial and Linear Regression, but sensitive to `k`.\n",
    "\n",
    "---\n",
    "\n",
    "### Regression Trees\n",
    "- Depth = 3 gave reasonable results.  \n",
    "- Depth = 7 with min_samples_leaf = 2 overfitted, performed poorly on the test set.  \n",
    "- Adjusting max_leaf_features further reduced accuracy.  \n",
    "- **Conclusion**: Performed moderately but did not generalize well.\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest\n",
    "- **Best Results**: 150 estimators with depth 10 gave good results.  \n",
    "- Increasing estimators beyond 150 or depth beyond 10 decreased accuracy.  \n",
    "- **Conclusion**: Worked well with default settings, though feature processing could improve results.\n",
    "\n",
    "---\n",
    "\n",
    "### AdaBoost\n",
    "- **Best Results**: RMSE = 12,849,063.35 with learning rate = 0.2 and linear loss.  \n",
    "- Increasing estimators slightly dropped accuracy; exponential loss took longer with no significant benefit.  \n",
    "- **Conclusion**: Performed better than Polynomial and Linear Regression but was not as effective as Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting\n",
    "- **Best Results**: 150 estimators, learning rate = 0.05: RMSE = 12,695,046.73.  \n",
    "- Increasing estimators or learning rate degraded performance.  \n",
    "- **Conclusion**: Competitive with Random Forest and highly effective.\n",
    "\n",
    "---\n",
    "\n",
    "### XGBoost\n",
    "- **Best Results**: Depth = 4, estimators = 400, learning rate = 0.015.  \n",
    "- Increasing estimators to 1000 reduced accuracy.  \n",
    "- **Conclusion**: Worked exceptionally well with fine-tuned parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### Neural Networks\n",
    "- **Performance**: Did not perform well; hidden layer sizes > 100 or iterations > 50 reduced performance.  \n",
    "- Activation functions (`ReLU`, `tanh`) had minimal impact.  \n",
    "- **Conclusion**: Not effective for this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Stacking\n",
    "- Ensemble of:\n",
    "  - XGBoost (125 estimators, learning rate = 0.01)\n",
    "  - Extra Trees (200 estimators)\n",
    "  - Linear Regressor as the final meta-model  \n",
    "  - **Performance**: RMSE = 12,440,977.00  \n",
    "- Stacking with Decision Trees (depth = 25) and Linear Regression resulted in RMSE = 12,593,651.63 but was computationally expensive.  \n",
    "- **Conclusion**: Promising results but time-consuming.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Preprocessing\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "- PCA with 50 components showed only a marginal improvement in accuracy.  \n",
    "- Dropping 10â€“20 features via PCA did not significantly help.  \n",
    "- **Conclusion**: Not very effective.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Importance & Selection\n",
    "- P-value-based feature selection showed no substantial improvement.  \n",
    "- Dropping irrelevant columns (e.g., `sub_area`) improved performance slightly.  \n",
    "- Forward and backward selection methods were computationally expensive with no significant gains.  \n",
    "- **Conclusion**: Limited success with feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "### Scaling & Normalization\n",
    "- Scaling with StandardScaler or MinMaxScaler did not improve performance significantly.  \n",
    "- **Conclusion**: No substantial impact.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Missing Values\n",
    "- No missing values in the dataset; no imputations were necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### Label Encoding\n",
    "- Categorical columns encoded using `LabelEncoder` with mapping:\n",
    "  - `Yes` â†’ 1  \n",
    "  - `No` â†’ 0  \n",
    "- Significant improvement in performance after encoding these columns:  \n",
    "  `['culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion', 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', 'big_road1_1line', 'railroad_1line', 'water_1line']`.\n",
    "\n",
    "---\n",
    "\n",
    "## Challenges\n",
    "\n",
    "1. **Finding the Right Model**  \n",
    "   - The first challenge was identifying the best-performing model. I tackled this by **randomly testing multiple models** and selecting the one with the most promising results (Extra Trees Regressor).  \n",
    "\n",
    "2. **Tuning Parameters**  \n",
    "   - Finding the optimal parameters was another hurdle. I approached this by **iteratively testing different configurations** of the chosen model. For instance, with Extra Trees, I experimented with various numbers of estimators, split criteria, and other parameters to maximize performance.\n",
    "\n",
    "3. **Decision on Individual vs. Ensemble Modeling**  \n",
    "   - A major decision point was whether to:\n",
    "     - Focus on an **individual model**.\n",
    "     - Attempt **ensemble modeling** for potential improvements.  \n",
    "   - I initially tried stacking two basic models and observed a **drop in accuracy**. Given the **time constraints of the competition**, I quickly decided to focus on enhancing the individual model instead of investing time in ensembling.  \n",
    "\n",
    "4. **Quick Decision-Making**  \n",
    "   - Despite the challenges, I made swift decisions to streamline the workflow, ultimately achieving a significant performance boost with the individual Extra Trees Regressor.  \n",
    "   \n",
    "**Result**: This strategic shift proved successful, leading to the discovery of the winning model! ðŸŽ‰\n",
    "\n",
    "\n",
    "## The Winner of the Competition: Extra Trees Regressor\n",
    "\n",
    "### Why Extra Trees Regressor?\n",
    "The **Extra Trees Regressor (Extremely Randomized Trees)** emerged as the best-performing model for the Russian housing price prediction. This model excels due to its use of **three levels of randomness**:\n",
    "1. **Bootstrapping**: Generates diverse samples from the training data, promoting robust generalization.\n",
    "2. **Random Feature Selection**: Selects a subset of features for splits, minimizing overfitting risks.\n",
    "3. **Randomized Splits**: Randomizes split thresholds, reducing sensitivity to noise and complex patterns.\n",
    "\n",
    "These features make Extra Trees particularly effective for high-dimensional data while maintaining computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Best Configuration\n",
    "The configuration that achieved the best results:  \n",
    "```python\n",
    "ExtraTreesRegressor(\n",
    "    n_estimators=2000, \n",
    "    min_samples_split=4, \n",
    "    bootstrap=False, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "Root Mean squared error: 12366669.20\n",
    "R-squared = 0.671\n",
    "KAGGLE SCORE = 12418350.13179\n",
    "```\n",
    "### Model Evolution\n",
    "- Initially tested with **300 estimators**, yielding an **RMSE of 12,451,151.25**.  \n",
    "- Incrementally increased the number of estimators to **2000**, further improving performance without any signs of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Increasing Estimators Doesn't Cause Overfitting?\n",
    "Extra Trees leverages **averaging across a large number of trees with randomized splits**. This approach:  \n",
    "- **Reduces variance** in predictions.  \n",
    "- **Enhances generalization** by mitigating the impact of noisy or extreme data points.  \n",
    "- **Stabilizes the model** as the number of estimators increases.  \n",
    "\n",
    "Thus, increasing estimators improves robustness and consistency without compromising accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Verdict\n",
    "The **Extra Trees Regressor** delivered outstanding performance with excellent generalization capabilities, making it the **WINNER** for the Russian housing price prediction dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
